<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Linear Regression | Curious...</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/articles/>Articles</a></li><li><a href=/index.xml>RSS</a></li></ul><hr></nav><div class=article-meta><h1><span class=title>Linear Regression</span></h1><h2 class=date>2021/05/04</h2></div><main><p>Hello. Today I want to write about Linear Regression. It is basically
the &ldquo;Hello, World&rdquo; program of Machine Learning. It is used to model
the relationship between one or more inputs and a single output. Its application
ranges from engineering to business. It is a process of learning from data.
It falls under supervised learning category of Maching Learning.
Regarding the algorithm, we have a set of inputs
\(X\) and their corresponding outputs \(Y\). From this,we try to come
up with a linear equation that best fits the data. Then, we can use that
equation to predict the output for new inputs.</p><p>There are mainly two ways to solve a linear regression problem. They
are</p><ul><li>Gradient Descent</li><li>Normal Equations</li></ul><p>Both of these will be explained in the article, but first, let us
state the problem a bit more clearly to avoid any confusions. Our
dataset consists of \(n+1\) items, where each item has a set of \(d\)
features, \(x_i\)&rsquo;s, and a corresponding output, \(y\). We can summarize
this information in following matrices.</p>$$ X_{n \times d} = \begin{bmatrix} (x^{(0)})^{T} \\ (x^{(1)})^{T}
\\ \vdots \\ (x^{(n-1)})^{T} \\ \end{bmatrix}
$$$$ x^{(i)}_{d\times 1} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots
\\ x_{d} \\ \end{bmatrix} $$$$ Y_{n \times 1} = \begin{bmatrix} y^{(0)} \\ y^{(1)} \\ \vdots
\\ y^{(n-1)} \\ \end{bmatrix} $$<p>\(X\) is the matrix of independent input variables. Each row of \(X\) is a vector
of features. The \(i^{th}\) element of \(Y\) is the corresponding output of \(i^{th}\) row of \(X\).
Our goal is to create a hypothesis function, \(h\), that maps the vector \(x\) to the vector \(y\). Since this is a linear
regression problem, it is obvious to think that the hypothesis
function produces its output as the /linear combination/ of its input,
i.e. the hypothesis function scales each feature by some amount and
sums them up including an additional constant term. We will denote the vector of scaling
factors and constant by \(\theta\). As there are \(d\) features, scaling
every feature would require \(d\) parameters. We also need a constant
term. So, in total, we need \(d+1\) thetas, subscripted from \(0\) to \(d\),
where \(\theta_0\) is the constant term. So, our parameter vector and hypothesis function
looks like this:</p>$$ \theta_{(d+1) \times 1} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_d \end{bmatrix} $$<p></p>$$ h(x) = \theta_0 + \sum_{i=1}^{d} \theta_{i} x_{i} $$<p>Now, our goal is to find the elements of \(\theta\).</p><h2 id=gradient-descent>Gradient Descent</h2><p>It is an algorithm to find these parameters. Here, we
define a loss function \(J(\theta)\), that is an indicator of the
degree of error present in our model. We define this function in such a way that
it captures inability of the model. The lower the value of
\(J(\theta)\), the better parameters we have. So, we will have
obtained the best choice for the elements of \(\theta\) if the error is as
low as possible. There are different choices of loss function.
One of the best choice for the loss function is the following:</p>$$
J(\theta) = \frac{1}{2n} \sum_{i=1}^{n} (h(x^{(i)}) - y^{(i)})^{2}
$$<p>It is called squared error loss function. It effectively captures the error present in our model
because each term in the series is the squared difference between the
true value of \(y\) and the prediction made by the model, \(h(x)\). Hence, larger the
difference, larger the error.</p><p>Now our goal is to minimize the value of the loss function of our model.
Since this is a minimization problem, we can use
calculus to make our work easier. Remember that our goal is to adjust
the parameters, \(\theta\) to attain the values for which the loss function will be minimum. So, we take the
derivative of the cost function, \(J(\theta)\) with respect to each
\(\theta\) and then update the value of each \(\theta\) to make
\(J(\theta)\) smaller. The loss function we have defined above is the
loss considering all the training examples. For a single training
example, the cost is</p>$$ J_i(\theta) = \frac{1}{2n} (h(x^{(i)}) - y^{(i)})^{2} $$<p>The derivative of this function with respect to a particular
\(\theta_{j}\) is</p>$$ \frac{\partial }{\partial \theta_{j}} J_i(\theta) =
\frac{\partial}{\partial \theta_{j}} \frac{1}{2n} (h(x^{(i)}) -
y^{(i)})^2 $$<p>We must update the value of \(\theta{j}\):</p>$$
\theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial
\theta_{j}} J(\theta)
$$<p></p>$$
\theta_{j} := \theta_{j} + \frac{\alpha}{n}
(y^{(i)} - h(x^{(i)})) \space x_{j}^{(i)}
$$<p>This should be done for all the values of \(j\). Here, \(\alpha\) is the learning rate.
It is the step size taken to update the value of \(\theta\). Suitable value of \(\alpha\) should
be chosen. Choosing a small value may result in slow convergence whereas too large
value may not result in convergence. There are two ways of updating the values.</p><p>If we update the value of a single theta considering the all the
training examples at once. This is called batch gradient descent. The
algorithm for this is</p>$$Repeat\space until\space convergence\space$$<p></p>$$ \theta_{j} :=
\theta_{j} + \frac{\alpha}{n} \sum_{i=1}^n \space(y^{(i)} - h(x^{(i)})) \space
x_{j}^{(i)}, \quad (for\space every\space j) $$<p>On the other hand, if we updae the values of \(\theta\)s by only looking
at a single training example, it is called stochastic gradient
descent. It is implemented as:</p>$$Loop\ for\ i=1\ to\ n $$<p></p>$$ \theta_{j}
:= \theta_{j} + \frac{\alpha}{n} (y^{(i)} - h(x^{(i)})) \space x_{j}^{(i)},
\quad (for\space every\space j) $$<p>So, among batch GD and stochastic GD, which one should we use then?
Upon looking the algorithm of batch GD, we find that there&rsquo;s a
significant calculation before updating the values of theta as we have
to calculate the summation term. This method is slower than
stochastic GD which updates the value of \(\theta\) looking at just one
training example at a time. This can speed up convergence. Loops are slow. For faster
calculation, we must <a href=https://en.wikipedia.org/wiki/Vectorization>vectorize</a> the operations as much as possible. I will show this in
the implementation section.</p><h2 id=normal-equations>Normal Equations</h2><p>We can solve the problem of Linear Regression even without using calculus.
As above, our dataset consists of \(X\), the matrix of independent variables and
\(Y\), the matrix of outputs. We have to find the \(\theta\)&rsquo;s for which following equation
satisfies:</p>$$
\begin{bmatrix}1 & (x^{(0)})^{T}\\1 & (x^{(1)})^{T}\\\vdots\\1 & (x^{(n-1)})^{T}\\\end{bmatrix}
\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\theta_d\end{bmatrix} =
\begin{bmatrix}y^{(0)}\\y^{(1)}\\\vdots\\y^{(n-1)}\end{bmatrix}
$$<p></p>$$ A \quad \quad \quad \quad \quad \theta\quad =\quad Y $$<p>The matrix \(A\) is a matrix formed by augmenting a column with all
elements 1 in front of matrix X. Alas! This system of equation will
hardly have an exact solution, because the probability of all our data
sets lying exactly on our fitting line is near to zero. So, what
should we do then? <em>We must project the vector \(Y\) onto the column
space of \(A\) to make this system solvable.</em> That may sound like a complicated process and you might even
be wondering <em>&ldquo;What&rsquo;s a column space?&rdquo;</em>. If you are, you need to understand some terminologies first.
Next section will be dedicated to familiarizing you with these terms. After that,
we&rsquo;ll proceed towards understanding this process. If you are already
familiar with linear algebra, you can skip next section. Remember that
these are not accurate mathematical definitions but some loose definitions that&rsquo;ll be sufficient
for our purpose.</p><h2 id=some-terms>Some Terms</h2><p><a href=https://en.wikipedia.org/wiki/Linear_combination><strong>Linear Combination</strong></a> of a set of vectors is an expression constructed
by multiplying each vector in the set by some constant and adding the
results. For example, a linear combination of \(\vec{i}\), unit vector
in x direction, and \(\vec{j}\), unit vector in y direction would be
\(a\vec{i} + b\vec{j}\).</p><p>A set of vectors are <a href=https://en.wikipedia.org/wiki/Linear_independence><strong>Linearly Independent</strong></a> if there is a linear
combination of the vectors equals zero vector, then each of the
multiplying factors must be zero. For example, vectors \(\vec{i}\) and
\(\vec{j}\) are independent, because no linear combination \(a\vec{i} +
b\vec{j}\) would produce zero vector, unless \(a = 0\) and \(b = 0\).</p><p><a href=https://en.wikipedia.org/wiki/Vector_space><strong>Vector Space</strong></a> is a set of vectors closed under
linear combination. That is, the linear combination of vectors inside
the vector space will produce a vector which is also inside the
space. \(\mathcal{R}^{N}\) is a vector space because any vector
resulting from linear combination of the vectors inside
\(\mathcal{R}^{N}\) will stay inside \(\mathcal{R}^{N}\)</p><p>A <a href=https://en.wikipedia.org/wiki/Linear_subspace><strong>Subspace</strong></a> is a subset of some larger vector space. A plane passing
through origin can be considered a subspace inside
\(\mathcal{R}^{3}\). It is because it is a subset of \(\mathcal{R}^{3}\)
and linear combination of any number of vectors lying inside the plane
will result in a vector which also lies inside it.</p><p>A <a href=https://en.wikipedia.org/wiki/Linear_span><strong>Linear Span</strong></a> of a set of vectors is the subspace that is formed by
linear combination of all the vectors in the set. For example, For
\(S = \{\vec{i}, \vec{j}\}\), the span is \(\mathcal{R}^{2}\) because from
the linear combination of \(\vec{i}\) and \(\vec{j}\), every vector in
\(\mathcal{R}^{2}\) can be constructed. In other words, these two vectors
<em>reach</em> entire \(\mathcal{R}^{2}\).</p><p>The <a href=https://en.wikipedia.org/wiki/Basis_(linear_algebra)><strong>basis vectors</strong></a> are the set of vectors which are <em>just enough</em> to span the
subspace. Not too few, not too many. The basis of \(\mathcal{R}^{2}\) can be
the set of \(\begin{bmatrix}1\\2\end{bmatrix}\) and \(\begin{bmatrix}3\\4\end{bmatrix}\)
because they can span the entire \(\mathcal{R}^{2}\). If we add one more element to
the set, for example, \(\begin{bmatrix}0\\1\end{bmatrix}\), that would be too many, we cannot
call it a basis. Or if we remove any one element, we couldn&rsquo;t call that a basis either.
Because a single vector cannot span \(\mathcal{R}^2\).</p><p>The <a href=https://en.wikipedia.org/wiki/Row_and_column_spaces><strong>Column Space</strong></a> of a matrix is the subspace formed by the column vectors
of the matrix. For a matrix \(\begin{bmatrix}1 & 3\\2 & 4\end{bmatrix}\),
its columnspace is \(\mathcal{R}^{2}\). Since the columns of this matrix,
\(\begin{bmatrix}1 \\ 2 \end{bmatrix}\) and \(\begin{bmatrix}3 \\ 4 \end{bmatrix}\) can /reach/ the entire
\(\mathcal{R}^{2}\). Whereas, for a matrix \(\begin{bmatrix}1 & 0 & 1\\0 & 1 & 1\\0 & 0 & 0\end{bmatrix}\),
the columnspace is the \(x-y\) plane inside \(\mathcal{R}^{3}\). It is not \(\mathcal{R}^{3}\)
because the linear combination of all columns only yields the vector of the
form \(\begin{bmatrix}a\\b\\0\end{bmatrix}\). The third column can be constructed
as \(1 \cdot (Column 1) + 1 \cdot (Column 2)\). So it does not help in the construction
of columnspace.</p><h2 id=the-meaning-of>The meaning of \(Ax = b\)</h2><p>Now let&rsquo;s understand the meaning of \(Ax = b\). Here \(A\) is a matrix,
\(x\) is a vector and \(b\) is another vector.
The matrix multiplication \(Ax\) is essentially the linear
combination of the columns of \(A\). Here&rsquo;s a little demonstration</p>$$
A x = b
$$$$
\begin{bmatrix}
a_{11} & a_{12} & \dots \\
\vdots & \ddots & \\
a_{m1} & & a_{mn}
\end{bmatrix}
\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix} =
\begin{bmatrix}b_1\\b_2\\\vdots\\b_n\end{bmatrix}
$$$$ x_1 \begin{bmatrix}a_{11}\\\vdots\\a_{m1}\end{bmatrix} +
x_2 \begin{bmatrix}a_{12}\\\vdots\\a_{m2}\end{bmatrix} + \dots +
x_n \begin{bmatrix}a_{1n}\\\vdots\\a_{mn}\end{bmatrix}
= \begin{bmatrix}b_1\\b_2\\\vdots\\b_n\end{bmatrix} $$<p>So, when we&rsquo;re solving for the system \(Ax = b\), we are actually finding out the
combination of the columns of \(A\) that will combine to form \(b\). For the solution to exist,
\(b\) must be in the columnspace of \(A\) otherwise no combination of the columns of \(A\) can yield \(b\).
And if no solution exists, as in the case of Normal Equations, we can still find the /best/ answer
by projecting \(b\) onto the columnspace of \(A\) and solving for the \(x\) which will combine to form the
projection. So, next we need to understand about projection.</p><h2 id=projection>Projection</h2><p>Let&rsquo;s say we want to project a vector \(\vec{b}\) onto a vector \(\vec{a}\). How do we do this?
First, let&rsquo;s denote some terms to make it easier. We&rsquo;ll refer to the projection of \(\vec{b}\) onto
\(\vec{a}\) as \(\vec{p}\). We also draw a vector from \(\vec{p}\) to the tip of vector \(\vec{b}\).
It will be called /error/ vector. It is \(\vec{e} = \vec{b} - \vec{p}\). Let&rsquo;s compute
\(\vec{p}\). We know that \(\vec{p}\) is a multiple of \(\vec{a}\) as it lies in the same line as \(\vec{a}\).
So, we can write \(\vec{p} = \hat{x}\vec{a}\). Since \(\vec{a}\) and \(\vec{e}\) are perpendicular to each
other, we can write</p><figure><img src=/images/articles/linear_regression/projection_vector.png><figcaption>Projection of a vector onto another</figcaption>&nbsp;</figure>$$
\vec{a} \cdot \vec{e} = 0
$$$$
\vec{a} \cdot (\vec{b} - \vec{p}) = 0
$$$$
\vec{a} \cdot (\vec{b} - \hat{x} \vec{a}) = 0
$$<p>On simple calculation, it yields</p>$$
\hat{x} = \frac{\vec{a} \cdot \vec{b}}{\vec{a} \cdot \vec{a}} = \frac{a^{T} \cdot b}{a^{T} \cdot a}
$$<p>We can substitute dot product for vectors by multiplicaton of transpose of one with the other. It&rsquo;s a really
trivial calculation. Try it out for yourself if you&rsquo;re still confused.</p><p>Now that we&rsquo;ve understood the basics of projection, we&rsquo;ll try to understand the process of projecting
a vector onto the column space of a matrix. It is not much different from what we did in the section above.
There we projected a vector \(\vec{b}\) onto a single vector \(\vec{a}\). Here, instead of a single vector to project onto,
we&rsquo;ll have multiple vectors, precisely the columns of the given matrix. Finding the projection of a vector
onto a column space means finding a linear combination of the columns which is closest to the vector
we&rsquo;re projecting. Let us say that \(A = \begin{bmatrix}\vec{a_1} \space \vec{a_2} \space \dots \space \vec{a_n} \end{bmatrix}\). The
\(\vec{a_i}\)&rsquo;s are the columns of \(A\). We&rsquo;re projecting \(\vec{b}\) onto the column space of \(A\).
Let&rsquo;s denote that projection by \(\vec{p}\). The error vector is \(\vec{e} = \vec{b} - \vec{p}\)
The projection \(\vec{p}\) must be some sombination of the columns of \(A\), i.e.</p><figure><img src=/images/articles/linear_regression/projection_subspace.png><figcaption>Projection of a vector onto a subspace S</figcaption>&nbsp;</figure>$$
\vec{p} = \hat{x}_1 \vec{a}_1 + \hat{x}_2 \vec{a}_2 + ... + \hat{x}_n \vec{a}_n
$$$$
\vec{p} = A \vec{x}
$$<p>Here \(\vec{x}\) is the vector whose elements are the \(\hat{x_i}\)&rsquo;s. From the above equation we can write the
error vector as \(\vec{e} = \vec{b} - A \vec{x}\). To find the \(\hat{x}\)&rsquo;s, we observe that as in the case of the line,
the error vector is perpendicular to the entire subspace. It also means that it is perpendicular to each of the vectors in the subspace.
So we can write</p>$$
a_1^{T} (\vec{b} - A \vec{x})
$$$$
a_2^{T} (\vec{b} - A \vec{x})
$$$$
\vdots
$$$$
a_n^{T} (\vec{b} - A \vec{x})
$$<p>This system of \(n\) equations can be described in terms of the following matrix equation</p>$$
A^{T} (\vec{b} - A \vec{x}) = 0
$$<p>On solving this, we get</p>$$
\vec{x} = (A^{T}A)^{-1}A^{T} \vec{b}
$$<p>And the projection of \(\vec{b}\) onto the column space of \(A\) is</p>$$
\vec{p} = A \vec{x} = A (A^{T}A)^{-1}A^{T} \vec{b}
$$<p>Now here&rsquo;s the final part of this whole calculation. Instead of solving for \(A \theta = Y\), we project
\(Y\) onto the column space of A and solve for it.
The projection of \(Y\) onto the column space of \(A\) is \(A \theta = A (A^{T}A)^{-1}A^{T} Y\).\(\quad\) So,
we solve for \(\theta\) as follows</p>$$
A \theta = A (A^{T}A)^{-1}A^{T}Y
$$$$
\theta = A^{-1} A (A^{T}A)^{-1}A^{T}Y
$$$$
\theta = (A^{T}A)^{-1}A^{T}Y
$$<p>Here&rsquo;s our parameters, \(\theta\). But one last question remains. What if \(A\) is singular? It means ()A
has no inverse. this case, we calculate the
<a href=https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse>pseudo inverse</a> of \(A^{T} A\) instead of inverse. Pseudo inverse is an another big topic that&rsquo;ll require an entire
article to explain. So, to keep this article short, I&rsquo;m not going into that detail. In implementation, we should use
functions like <code>numpy.linalg.pinv</code> in python, or <code>pinv</code> in matlab.</p><h2 id=implementation>Implementation</h2><h3 id=coding>Coding</h3><p>This is our initial data</p><img src=/images/articles/linear_regression/initial.png><h4 id=gradient-descent-1>Gradient Descent</h4><p>You need to read the <code>X</code> and <code>Y</code> variables from some file using a library like [[https://pandas.pydata.org/][Pandas]] and convert them to
numpy array with appropriate dimensions. Also, you need to augment the <code>X</code> variable with a column of <code>1</code> to make the number
of columns of <code>X</code> same as that of the number of rows of <code>theta</code>. Here is a gradient
descent function for linear regression in python using <code>numpy</code>.</p><pre><code class=language-python>import numpy as np

def gradient_descent(X, Y, theta, learning_rate, num_iters):
    n = np.shape(X)[0]
    for i in range(num_iters):

        # calculate the current prediction
        prediction = np.matmul(X, theta)

        # obtain the error
        error_vector = Y - prediction

        # Update the thetas. Here I've vectorized the operation of updating thetas
        # try matching dimensions of multiplied matrices to understand the operation
        theta = theta + (learning_rate/n) * np.matmul(np.transpose(X), error_vector)

        return theta
</code></pre><p>Here is the result from gradient descent running on my computer:</p><img src=/images/articles/linear_regression/gradient.png><h4 id=normal-equations-1>Normal Equations</h4><p>This algorithm is easier than the Gradient Descent one. It is only the matter of matrix manipulation.
Here is a python function for normal equations.</p><pre><code class=language-python>  import numpy as np

  def normal_equations(X, Y):

      # Calculate the transpose
      X_T = np.transpose(X)

      # This step can be combined with the step below
      # This is done to make the return statement concise.
      intermediate = np.linalg.pinv(np.matmul(X_T, X))

      return np.matmul(np.matmul(intermediate, X_T), Y)
</code></pre><p>This predictor is what normal equations yields on my machine:</p><img src=/images/articles/linear_regression/normal_eqn.png><p>As you can see both algorithms perform equally good. It is because our data set is small(only about a hundred items)
and gradient descent
reaches the minimum value quickly. And I&rsquo;ve used sufficient number of iterations \((> 2000)\) for our purpose.
On large dataset, it may be a little less accurate than the Normal Equations. In that case, you&rsquo;d need to increase
the number of iterations.</p><h3 id=how-to-make-a-higher-dimensional-predictor>How to make a higher dimensional predictor?</h3><p>We should generate more features on \(X\). If we only have one dimensional input, we can make it two dimensional by
mapping the first column to a square function and using the output as a second column of X. Then our model will provide us with
three parameters, the coefficients of each degree term in a quadratic equation.</p><h3 id=which-one-is-better>Which one is better?</h3><p>For a system with \(d\) features and \(n\) data items, the time complexity of gradient descent is \(O(n^2 d)\) whereas that of
the normal equations is \(O(n^3)\). It is pretty obvious that for smaller datasets, you&rsquo;d use normal equation as it
provides you with more accurate result than gradient descent. But for large datasets, especially with number of features
less than the number of data items, gradient descent is the only good choice.</p><h2 id=closing-words>Closing Words</h2><p>This program is the entry point of any student to Machine Learning. It may not feel like much (or it may, depending upon the
reader 😉) but it gives you a speak peek into the paradigm of Machine Learning. This is what let&rsquo;s our computers see and talk.
Isn&rsquo;t it amazing?</p></main><footer><link rel=stylesheet href=//cdn.jsdelivr.net/npm/katex/dist/katex.min.css><script src=//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js defer></script><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js defer></script><hr>© Udeshya Dhungana 2020 - Today | <a href=https://github.com/udeshyadhungana>Github</a> | <a href=https://x.com/udeshyadhungana>Twitter</a></footer></body></html>